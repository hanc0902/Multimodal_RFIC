{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0639e0a2-a15f-40e8-aaad-413480e7e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201110c6-a521-413e-87b8-1e9a24b2ddf6",
   "metadata": {},
   "source": [
    "### Fusion Encoder Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec9836ae-d244-4f8d-82f4-4160e863bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77ca5d94-18c3-4684-812d-e19c88539a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "testlayer = nn.TransformerEncoderLayer(d_model=512, nhead=8, activation=\"gelu\")\n",
    "test = nn.TransformerEncoder(testlayer, num_layers=6).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7aa347c5-93fd-49ee-abfe-1fc0c3557b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in function gelu>\n",
      "TransformerEncoder(\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(test.layers[0].activation)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6a3f080-d474-494f-84b5-013652787a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Individual bottleneck layer\n",
    "class BottleneckLayer(nn.Module):\n",
    "    def __init__(self, num_latents, schem_enc, graph_enc, tab_enc): #add encoders if modified to include attn and mlp and such\n",
    "        super(BottleneckLayer, self).__init__()\n",
    "\n",
    "        # SCHEMATIC\n",
    "        self.schem_encoder = schem_enc\n",
    "        \n",
    "        # GRAPH\n",
    "        self.graph_encoder = graph_enc\n",
    "\n",
    "        # TABULAR\n",
    "        self.tab_encoder = tab_enc\n",
    "        \n",
    "        # Latents\n",
    "        self.num_latents = num_latents\n",
    "        self.latents = nn.Parameter(torch.empty(1,num_latents,512).normal_(std=0.02)) #512 to match dimensionality\n",
    "        self.scale_s = nn.Parameter(torch.zeros(1))\n",
    "        self.scale_g = nn.Parameter(torch.zeros(1))\n",
    "        self.scale_t = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "\n",
    "    def attention(self,q,k,v): # requires q,k,v to have same dim. In future I want multi head self attention (MSA)\n",
    "        B, N, C = q.shape\n",
    "        attn = (q @ k.transpose(-2, -1)) * (C ** -0.5) # scaling\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).reshape(B, N, C)\n",
    "        return x\n",
    "    \n",
    "    # Latent Fusion\n",
    "    def fusion(self, schem_tokens, graph_tokens, tab_tokens):\n",
    "        # shapes\n",
    "        BS = schem_tokens.shape[0]\n",
    "        # concat all the tokens\n",
    "        concat_ = torch.cat((schem_tokens, graph_tokens, tab_tokens),dim=1)\n",
    "        # cross attention (modalities -->> latents)\n",
    "        fused_latents = self.attention(q=self.latents.expand(BS,-1,-1), k=concat_, v=concat_)\n",
    "        # cross attention (latents -->> modalities)\n",
    "        schem_tokens = schem_tokens + self.scale_s * self.attention(q=schem_tokens, k=fused_latents, v=fused_latents)\n",
    "        graph_tokens = graph_tokens + self.scale_g * self.attention(q=graph_tokens, k=fused_latents, v=fused_latents)\n",
    "        tab_tokens = tab_tokens + self.scale_t * self.attention(q=tab_tokens, k=fused_latents, v=fused_latents)\n",
    "        return schem_tokens, graph_tokens, tab_tokens\n",
    "    \n",
    "    def forward(self, x, y, z):\n",
    "\n",
    "        # Bottleneck Fusion\n",
    "        x,y,z = self.fusion(x,y,z)\n",
    "\n",
    "        x = schem_encoder(x)\n",
    "        y = graph_encoder(y)\n",
    "        z = tab_encoder(z)\n",
    "        \n",
    "        return x,y,z, self.latents.expand(BS,-1,-1)\n",
    "        \n",
    "#####################################################################################################################################################\n",
    "#####################################################################################################################################################\n",
    "#####################################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b54dd5-b3ef-4cd8-8bf2-29e12b1a3727",
   "metadata": {},
   "source": [
    "### Unimodal Encoder Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2068665f-8771-47e7-921c-378f237b774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schematic (CNN)\n",
    "class SchemEncoder(nn.Module):\n",
    "    def __init__(self, fmax):\n",
    "        super(SchemEncoder, self).__init__()\n",
    "        \n",
    "        # Define the CNN part for input 1 (224x224x3)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, n_channel, kernel_size=3, stride=1, padding=1),  # (224, 224, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (112, 112, 32)\n",
    "            nn.Conv2d(n_channel, n_channel*2, kernel_size=3, stride=1, padding=1),  # (112, 112, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (56, 56, 128)\n",
    "            nn.Conv2d(n_channel*2, n_channel*4, kernel_size=3, stride=1, padding=1),  # (56, 56, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (28, 28, 128)\n",
    "            nn.Conv2d(n_channel*4, n_channel*4, kernel_size=3, stride=1, padding=1),  # (28, 28, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (14, 14, 128)\n",
    "            nn.Conv2d(n_channel*4, n_channel*4, kernel_size=3, stride=1, padding=1),  # (14, 14, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (7, 7, 128)\n",
    "            nn.Flatten(),  # Flatten to (7 * 7 * 128)\n",
    "            nn.Linear(7 * 7 * n_channel*4, 512),  # Fully connected layer\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Define the output layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, fmax * 12),  # Concatenate with input 2 (12 features)\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.reshape = nn.Unflatten(1, (fmax, 12))  # Reshape to (fmax, 12)\n",
    "\n",
    "    def forward(self, x1):\n",
    "        # Process input 1 through CNN\n",
    "        x = self.cnn(x1)\n",
    "        # Concatenate with input 2\n",
    "        #x = torch.cat((x1, x2), dim=1)\n",
    "        # Fully connected layer and reshape\n",
    "        x = self.fc(x)\n",
    "        x = self.reshape(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "add83ab7-d843-40bf-952d-2f68c3059fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph (Graph Transformer)\n",
    "\n",
    "def get_activation(activation_str):\n",
    "    \"\"\"Map string to activation function.\"\"\"\n",
    "    if activation_str.lower() == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation_str.lower() == \"gelu\":\n",
    "        return F.gelu\n",
    "    # Add more activations if needed\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation: {activation_str}\")\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    One Transformer encoder block, matching the structure of:\n",
    "      1) Multi-head self-attention (with dropout)\n",
    "      2) Feed-forward network (two linear layers + activation + dropout)\n",
    "      3) LayerNorm + residual connections\n",
    "      4) Optional \"pre-norm\" (norm_first=True) vs \"post-norm\" (norm_first=False)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size,\n",
    "        num_attention_heads,\n",
    "        intermediate_size,\n",
    "        activation=\"relu\", #check with houbo which activation is the best for GT block performance\n",
    "        dropout_rate=0.0,\n",
    "        attention_dropout_rate=0.0,\n",
    "        use_bias=False,\n",
    "        norm_first=True,\n",
    "        norm_epsilon=1e-6,\n",
    "        intermediate_dropout=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.attention_dropout_rate = attention_dropout_rate\n",
    "        self.use_bias = use_bias\n",
    "        self.norm_first = norm_first\n",
    "        self.norm_epsilon = norm_epsilon\n",
    "        self.intermediate_dropout = intermediate_dropout\n",
    "\n",
    "        # ---- Self-Attention ----\n",
    "        # nn.MultiheadAttention expects shape: [seq_len, batch_size, embed_dim]\n",
    "        # bias = `use_bias` is not directly exposed in nn.MultiheadAttention;\n",
    "        # PyTorch always learns a bias in the projection layers. If you want\n",
    "        # to remove bias, you must create a custom multi-head attention layer.\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_attention_heads,\n",
    "            dropout=attention_dropout_rate,\n",
    "            batch_first=False,  # We'll reshape manually\n",
    "            # PyTorch multi-head attention includes biases by default.\n",
    "            # For a strictly \"no-bias\" version, you'd need a custom approach.\n",
    "        )\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "        self.attention_layer_norm = nn.LayerNorm(hidden_size, eps=norm_epsilon)\n",
    "\n",
    "        # ---- Feed-Forward Network (FFN) ----\n",
    "        self.intermediate_dense = nn.Linear(hidden_size, intermediate_size, bias=use_bias)\n",
    "        self.intermediate_act_fn = get_activation(activation)\n",
    "        self.intermediate_dropout_layer = nn.Dropout(intermediate_dropout)\n",
    "\n",
    "        self.output_dense = nn.Linear(intermediate_size, hidden_size, bias=use_bias)\n",
    "        self.output_dropout = nn.Dropout(dropout_rate)\n",
    "        self.output_layer_norm = nn.LayerNorm(hidden_size, eps=norm_epsilon)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          hidden_states: Tensor of shape [batch_size, seq_len, hidden_size].\n",
    "          attention_mask: Optional tensor for attention, expected shape\n",
    "              [batch_size, seq_len, seq_len] with 0 for valid positions and\n",
    "              -inf (or large negative) for masked positions, or a boolean mask.\n",
    "              This may need to be adapted depending on how you've constructed\n",
    "              your mask. \n",
    "        Returns:\n",
    "          hidden_states: Tensor of shape [batch_size, seq_len, hidden_size].\n",
    "        \"\"\"\n",
    "\n",
    "        # --- Self-Attention block ---\n",
    "        # If norm_first, we layer-norm before attention; otherwise after\n",
    "        residual = hidden_states\n",
    "        if self.norm_first:\n",
    "            hidden_states = self.attention_layer_norm(hidden_states)\n",
    "\n",
    "        # Reshape hidden_states from [batch, seq, dim] to [seq, batch, dim]\n",
    "        hidden_states_t = hidden_states.transpose(0, 1)\n",
    "\n",
    "        # Convert mask if needed: PyTorch expects shape [seq_len, seq_len] or \n",
    "        # [batch_size * num_heads, seq_len, seq_len]. \n",
    "        # A simple approach is to expand so shape [batch, 1, seq, seq].\n",
    "        # Then internally PyTorch may broadcast it properly, or you\n",
    "        # can pass `attn_mask=some_mask` that is [seq, seq]. \n",
    "        # Here is an example that transforms the user’s [batch, seq, seq] \n",
    "        # into a float mask with -inf in invalid positions:\n",
    "        if attention_mask is not None:\n",
    "            # Suppose attention_mask=1 for valid, 0 for invalid, or the other way around.\n",
    "            # You may need to invert it, depending on how your mask is built.\n",
    "            # Here we assume \"1 = keep, 0 = mask out\".\n",
    "            attn_mask_pytorch = (1.0 - attention_mask) * -1e9\n",
    "            #print(attn_mask_pytorch.size())\n",
    "            # Expand dims if needed to [batch, 1, seq, seq], then flatten\n",
    "            # heads.  Alternatively, you can let PyTorch broadcast the shape.\n",
    "            # We’ll do a direct approach below:\n",
    "            #attn_mask_pytorch = attn_mask_pytorch.unsqueeze(1)  # [batch, 1, seq, seq]\n",
    "            #print(attn_mask_pytorch.size())\n",
    "        else:\n",
    "            attn_mask_pytorch = None\n",
    "\n",
    "        # Apply multi-head attention:\n",
    "        attn_output, _ = self.self_attention(\n",
    "            hidden_states_t,   # query\n",
    "            hidden_states_t,   # key\n",
    "            hidden_states_t,   # value\n",
    "            attn_mask=attn_mask_pytorch,\n",
    "        )\n",
    "\n",
    "        # Transpose back to [batch, seq, dim]\n",
    "        attn_output = attn_output.transpose(0, 1)\n",
    "\n",
    "        attn_output = self.attention_dropout(attn_output)\n",
    "        # Residual connection\n",
    "        hidden_states = residual + attn_output\n",
    "\n",
    "        if not self.norm_first:\n",
    "            hidden_states = self.attention_layer_norm(hidden_states)\n",
    "\n",
    "        # --- Feed Forward block ---\n",
    "        residual = hidden_states\n",
    "        if self.norm_first:\n",
    "            hidden_states = self.output_layer_norm(hidden_states)\n",
    "\n",
    "        # Intermediate (expand) + activation\n",
    "        hidden_states = self.intermediate_dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        hidden_states = self.intermediate_dropout_layer(hidden_states)\n",
    "\n",
    "        # Project back to hidden_size\n",
    "        hidden_states = self.output_dense(hidden_states)\n",
    "        hidden_states = self.output_dropout(hidden_states)\n",
    "\n",
    "        # Residual connection\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        if not self.norm_first:\n",
    "            hidden_states = self.output_layer_norm(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stacks N TransformerEncoderBlock layers and applies a final layer norm\n",
    "    (to match the original Keras code which has 'output_normalization').\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers=6,\n",
    "        num_attention_heads=8,\n",
    "        intermediate_size=2048,\n",
    "        activation=\"relu\",\n",
    "        dropout_rate=0.0,\n",
    "        attention_dropout_rate=0.0,\n",
    "        use_bias=False,\n",
    "        norm_first=True,\n",
    "        norm_epsilon=1e-6,\n",
    "        intermediate_dropout=0.0,\n",
    "        hidden_size=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          num_layers: Number of encoder layers.\n",
    "          num_attention_heads: Number of attention heads.\n",
    "          intermediate_size: Dim of the FFN's hidden layer.\n",
    "          activation: Activation for the intermediate (FFN) layer.\n",
    "          dropout_rate: Dropout probability for the output of each sub-layer.\n",
    "          attention_dropout_rate: Dropout probability for the attention scores.\n",
    "          use_bias: Whether linear layers use bias.\n",
    "          norm_first: If True, apply layer norm before each sub-block.\n",
    "          norm_epsilon: Epsilon for layer norm.\n",
    "          intermediate_dropout: Dropout within the feed-forward 'intermediate' layers.\n",
    "          hidden_size: The input/output hidden size. If None, derive from input.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.attention_dropout_rate = attention_dropout_rate\n",
    "        self.use_bias = use_bias\n",
    "        self.norm_first = norm_first\n",
    "        self.norm_epsilon = norm_epsilon\n",
    "        self.intermediate_dropout = intermediate_dropout\n",
    "\n",
    "        # You can either require hidden_size to be passed explicitly,\n",
    "        # or you can infer it at runtime (by passing the first batch through).\n",
    "        if hidden_size is None:\n",
    "            raise ValueError(\n",
    "                \"You must specify 'hidden_size' (the input feature dimension).\"\n",
    "            )\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(\n",
    "                hidden_size=hidden_size,\n",
    "                num_attention_heads=self.num_attention_heads,\n",
    "                intermediate_size=self.intermediate_size,\n",
    "                activation=self.activation,\n",
    "                dropout_rate=self.dropout_rate,\n",
    "                attention_dropout_rate=self.attention_dropout_rate,\n",
    "                use_bias=self.use_bias,\n",
    "                norm_first=self.norm_first,\n",
    "                norm_epsilon=self.norm_epsilon,\n",
    "                intermediate_dropout=self.intermediate_dropout,\n",
    "            ) for _ in range(self.num_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_normalization = nn.LayerNorm(hidden_size, eps=self.norm_epsilon)\n",
    "\n",
    "    def forward(self, encoder_inputs, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          encoder_inputs: shape [batch_size, seq_len, hidden_size].\n",
    "          attention_mask: shape [batch_size, seq_len, seq_len] or None.\n",
    "        Returns:\n",
    "          output shape [batch_size, seq_len, hidden_size].\n",
    "        \"\"\"\n",
    "        hidden_states = encoder_inputs\n",
    "\n",
    "        # Pass through each TransformerEncoderBlock\n",
    "        for i, layer in enumerate(self.encoder_layers):\n",
    "            #print(attention_mask.size())\n",
    "            hidden_states = layer(hidden_states, attention_mask=attention_mask)\n",
    "\n",
    "        # Final layer normalization (as in Keras code)\n",
    "        output_tensor = self.output_normalization(hidden_states)\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "#################################\n",
    "# Main Model\n",
    "#################################\n",
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self, transformer_encoder, fmax):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        self.transformer_encoder = transformer_encoder  # Use the pre-defined transformer model\n",
    "        self.fcl1 = nn.Linear(7, fdim)\n",
    "        self.fcl2 = nn.Linear(7, fdim)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(max_len*fdim*2, 512)  # Adjust input shape after concatenation 448*512\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 512)\n",
    "        self.fc5 = nn.Linear(512, 512)\n",
    "        self.out = nn.Linear(512, fmax * 12)\n",
    "        self.reshape = lambda x: x.view(-1, fmax, 12)  # Equivalent to `Reshape((fmax,12))` in Keras\n",
    "    def forward(self, inp1, inp2):\n",
    "        l1 = self.fcl1(inp1)\n",
    "        l2 = self.fcl2(inp2)\n",
    "        #print(l1.size())\n",
    "        #print(create_padding_mask(l1).size())\n",
    "        l1 = self.transformer_encoder(l1,create_padding_mask(inp1))\n",
    "        l2 = self.transformer_encoder(l2,create_padding_mask(inp2))\n",
    "\n",
    "        out = torch.cat((l1, l2), dim=1)  # Equivalent to `Concatenate()([l1, l2])`\n",
    "        out = self.flatten(out) #shape [8 2048]\n",
    "     #   print(\"1:\", out.shape)  \n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = torch.relu(self.fc2(out))\n",
    "        out = torch.relu(self.fc3(out))\n",
    "        out = torch.relu(self.fc4(out))\n",
    "        out = torch.relu(self.fc5(out))\n",
    "      #  print(\"2:\", out.shape)  \n",
    "        out = torch.tanh(self.out(out))  # Equivalent to `Dense(fmax*12, activation='tanh')`\n",
    "        out = self.reshape(out)\n",
    "       # print(\"3:\", out.shape)  \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3fff182-b00b-40f4-961d-5ed0eb3c3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tabular (MLP)\n",
    "# PyTorch model equivalent to Keras Sequential model\n",
    "class TabEncoder(nn.Module):\n",
    "    def __init__(self, fband, input_size=16):\n",
    "        super(TabEncoder, self).__init__()\n",
    "        self.fband = fband\n",
    "        self.fc1 = nn.Linear(input_size, 512)  # Equivalent to Dense(512) in Keras\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 512)\n",
    "        self.out = nn.Linear(512, fband * 12)  # Output layer\n",
    "        self.tanh = nn.Tanh()  # Equivalent to 'tanh' activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # First Dense layer with ReLU activation\n",
    "        x = F.relu(self.fc2(x))  # Second Dense layer\n",
    "        x = F.relu(self.fc3(x))  # Third Dense layer\n",
    "        x = F.relu(self.fc4(x))  # Fourth Dense layer\n",
    "        x = self.out(x)          # Output layer\n",
    "        x = self.tanh(x)         # Tanh activation for output\n",
    "        x = x.view(-1, self.fband, 12)  # Reshape to (fband, 12)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b77587-791f-454d-b480-25deff13b789",
   "metadata": {},
   "source": [
    "### Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d4f203ec-e132-44a1-866c-508b235d8287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "fmax = 100\n",
    "fstart = 0\n",
    "\n",
    "# Initialize PyTorch model and move to device\n",
    "nband = int(1)\n",
    "overlap = int(1)\n",
    "fband = int(fmax *overlap/ nband)  # Replace with your value for fmax\n",
    "bandslice = int(fband/overlap)\n",
    "fdim = 32\n",
    "max_len = 7 #idk what this is yet\n",
    "n_channel = 16\n",
    "\n",
    "models_dir = '/home/ch106/Desktop/ASP_DAC2026/MxN/models'\n",
    "pretrained_paths = { #dictionary storing pretrained weights for unimodal encoders\n",
    "    'schem_weights': models_dir + '/' +'cnn_1band/0.pth',\n",
    "    'graph_weights': models_dir + '/' +'GT_1band/0.pth',\n",
    "    'tab_weights': models_dir + '/' +'mlp_1band/0.pth'\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f80e82be-0713-4326-980f-daa1fa313e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to freeze parameters for unimodal heads (non-joint training scheme)\n",
    "\n",
    "def freeze(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "88493ba9-1ddc-4aec-8f43-989925d5c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, pretrained_paths=None, num_latents=4, dim=512):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "\n",
    "        #unimodal heads\n",
    "        self.v1 = SchemEncoder(fband)# for schematic\n",
    "        self.v2 = GraphEncoder(TransformerEncoder(intermediate_size=512,hidden_size=fdim), fmax=fband) # for graph\n",
    "        self.v3 = TabEncoder(fband)# for tabular\n",
    "\n",
    "\n",
    "        if pretrained_paths: #load pretrained weights\n",
    "            if 'schem_weights' in pretrained_paths:\n",
    "                self.v1.load_state_dict(torch.load(pretrained_paths['schem_weights'], strict =False))\n",
    "            if 'graph_weights' in pretrained_paths:\n",
    "                self.v2.load_state_dict(torch.load(pretrained_paths['graph_weights'], strict =False))\n",
    "            if 'tab_weights' in pretrained_paths:\n",
    "                self.v3.load_state_dict(torch.load(pretrained_paths['tab_weights'], strict =False))\n",
    "\n",
    "        \"\"\"\n",
    "        discard unnecessary layers and save parameters\n",
    "        \"\"\"\n",
    "        self.v1.fc = nn.Identity()\n",
    "        self.v1.reshape = nn.Identity()\n",
    "\n",
    "        self.v2.out = nn.Identity()\n",
    "        self.v2.reshape = nn.Identity()\n",
    "        \n",
    "        self.v3.out = nn.Identity()\n",
    "        self.v3.tanh = nn.Identity()\n",
    "        \n",
    "        \"\"\"\n",
    "        Freeze parameters (comment out for joint training scheme)\n",
    "        \"\"\"\n",
    "        freeze(self.v1)\n",
    "        freeze(self.v2)\n",
    "        freeze(self.v3)\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize auxillary unimodal transformers for fusion layers\n",
    "        \"\"\"\n",
    "        encoder_base = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        fusion_layers = 4\n",
    "        \n",
    "        self.schem_aux = nn.TransformerEncoder(encoder_base, num_layers=fusion_layers)\n",
    "        self.graph_aux = nn.TransformerEncoder(encoder_base, num_layers=fusion_layers)\n",
    "        self.tab_aux = nn.TransformerEncoder(encoder_base, num_layers=fusion_layers)\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize Fusion Encoder and spectral head\n",
    "        \"\"\"\n",
    "        encoder_layers = []\n",
    "        for i in range(fusion_layers):\n",
    "\n",
    "            # Vanilla Transformer Encoder (use for full fine tuning)\n",
    "            \n",
    "            encoder_layers.append(BottleneckLayer(num_latents=num_latents, schem_enc=self.schem_aux.layers[i],graph_enc=self.graph_aux.layers[i],tab_enc=self.tab_aux.layers[i]))\n",
    "\n",
    "            # Frozen Transformer Encoder with AdaptFormer \n",
    "            #encoder_layers.append(AdaptFormer(num_latents=num_latents, dim=dim, schem_enc=self.schem_aux.blocks[i], graph_enc=self.graph_aux.blocks[i], tab_enc=self.tab_aux.blocks[i]))\n",
    "             \n",
    "        self.fusion_blocks = nn.Sequential(*encoder_layers)\n",
    "\n",
    "        #add normalization of bottlenecks maybe?\n",
    "        \n",
    "        # spectral head\n",
    "        self.fc1 = nn.Linear(512,512)\n",
    "        self.fc2 = nn.Linear(512,512)\n",
    "        self.fc3 = nn.Linear(512,512)\n",
    "        self.fc4 = nn.Linear(512,512)\n",
    "        self.out = nn.Linear(512, fmax*12)\n",
    "        self.reshape = lambda x: x.view(-1, fmax, 12)\n",
    "\n",
    "    def forward_encoder(self,x,y,z):     \n",
    "        # encoder forward pass\n",
    "        for blk in self.fusion_blocks:\n",
    "            x,y,z, bottlenecks = blk(x,y,z)\n",
    "        return x,y,z, bottlenecks\n",
    "        \n",
    "    def forward(self, x, y, z):\n",
    "        #unimodal heads\n",
    "        x = self.v1(x)\n",
    "        y = self.v2(y)\n",
    "        z = self.v3(z)\n",
    "\n",
    "        #fusion encoders\n",
    "        x,y,z, bottlenecks = self.forward_encoder(x,y,z) #try unified transformer after this in future w/ concatenated tokens\n",
    "\n",
    "        #spectral head\n",
    "        out = torch.relu(self.fc1(bottlenecks))\n",
    "        out = torch.relu(self.fc2(out))\n",
    "        out = torch.relu(self.fc3(out))\n",
    "        out = torch.relu(self.fc4(out))\n",
    "        out = torch.tanh(self.out(out))\n",
    "        out = self.reshape(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d71423ef-4aa1-4982-8566-0a521802884e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultimodalModel(\n",
      "  (v1): SchemEncoder(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): ReLU()\n",
      "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU()\n",
      "      (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (10): ReLU()\n",
      "      (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU()\n",
      "      (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (15): Flatten(start_dim=1, end_dim=-1)\n",
      "      (16): Linear(in_features=3136, out_features=512, bias=True)\n",
      "      (17): ReLU()\n",
      "    )\n",
      "    (fc): Identity()\n",
      "    (reshape): Identity()\n",
      "  )\n",
      "  (v2): GraphEncoder(\n",
      "    (transformer_encoder): TransformerEncoder(\n",
      "      (encoder_layers): ModuleList(\n",
      "        (0-5): 6 x TransformerEncoderBlock(\n",
      "          (self_attention): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (attention_layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
      "          (intermediate_dense): Linear(in_features=32, out_features=512, bias=False)\n",
      "          (intermediate_dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "          (output_dense): Linear(in_features=512, out_features=32, bias=False)\n",
      "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (output_layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (output_normalization): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "    (fcl1): Linear(in_features=7, out_features=32, bias=True)\n",
      "    (fcl2): Linear(in_features=7, out_features=32, bias=True)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=448, out_features=512, bias=True)\n",
      "    (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (fc3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (fc4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (fc5): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out): Identity()\n",
      "    (reshape): Identity()\n",
      "  )\n",
      "  (v3): TabEncoder(\n",
      "    (fc1): Linear(in_features=16, out_features=512, bias=True)\n",
      "    (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (fc3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (fc4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out): Identity()\n",
      "    (tanh): Identity()\n",
      "  )\n",
      "  (schem_aux): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (graph_aux): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (tab_aux): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fusion_blocks): Sequential(\n",
      "    (0): BottleneckLayer(\n",
      "      (schem_encoder): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (graph_encoder): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (tab_encoder): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckLayer(\n",
      "      (schem_encoder): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (graph_encoder): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (tab_encoder): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): BottleneckLayer(\n",
      "      (schem_encoder): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (graph_encoder): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (tab_encoder): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): BottleneckLayer(\n",
      "      (schem_encoder): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (graph_encoder): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (tab_encoder): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc4): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out): Linear(in_features=512, out_features=1200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test = MultimodalModel()\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f2a36-e956-488e-bc11-0515b757db00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
