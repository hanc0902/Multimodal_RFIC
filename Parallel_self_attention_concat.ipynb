{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0639e0a2-a15f-40e8-aaad-413480e7e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca288eb-b2c0-4f06-aa41-96625d0f9fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 13:50:30.422674: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753469430.441828 3769732 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753469430.448266 3769732 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-25 13:50:30.469356: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import csv\n",
    "import math\n",
    "import pandas as pd\n",
    "import skrf as rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a84f352-3091-4684-b587-0296a6b0e216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(13, 139, [(-21.5, -232.5), (-21.5, -64.5)]), (np.float64(13.0), 39, [(np.float64(-15.0), np.float64(-71.0)), (np.float64(-32.0), np.float64(-71.0)), (np.float64(-89.0), np.float64(-14.0)), (np.float64(-89.0), np.float64(14.0)), (np.float64(-31.999999999999996), np.float64(71.0)), (np.float64(32.0), np.float64(71.0)), (np.float64(89.0), np.float64(14.0)), (np.float64(89.0), np.float64(-14.0)), (np.float64(31.999999999999996), np.float64(-71.0)), (np.float64(15.0), np.float64(-71.0))]), (np.float64(13.0), 39, [(np.float64(-15.0), np.float64(-106.0)), (np.float64(-46.49747468305833), np.float64(-106.0)), (np.float64(-124.0), np.float64(-28.497474683058336)), (np.float64(-123.99999999999999), np.float64(28.497474683058336)), (np.float64(-46.49747468305833), np.float64(106.0)), (np.float64(46.49747468305833), np.float64(106.0)), (np.float64(124.0), np.float64(28.497474683058336)), (np.float64(123.99999999999999), np.float64(-28.497474683058336)), (np.float64(46.49747468305833), np.float64(-106.0)), (np.float64(15.0), np.float64(-106.0))]), (np.float64(13.0), 39, [(np.float64(-15.0), np.float64(-141.0)), (np.float64(-60.99494936611666), np.float64(-141.0)), (np.float64(-159.0), np.float64(-42.99494936611666)), (np.float64(-159.0), np.float64(42.99494936611666)), (np.float64(-60.99494936611665), np.float64(141.0)), (np.float64(60.99494936611666), np.float64(141.0)), (np.float64(159.0), np.float64(42.99494936611666)), (np.float64(159.0), np.float64(-42.99494936611666)), (np.float64(60.99494936611665), np.float64(-141.0)), (np.float64(15.0), np.float64(-141.0))]), (13, 139, [(21.5, -64.5), (21.5, -232.5)])]\n",
      "5\n",
      "[(8, 139, [(-19.0, 232.5), (-19.0, 84.5)]), (np.float64(8.0), 39, [(np.float64(-15.0), np.float64(88.5)), (np.float64(-39.248737341529164), np.float64(88.5)), (np.float64(-106.5), np.float64(21.248737341529164)), (np.float64(-106.5), np.float64(-21.248737341529164)), (np.float64(-39.248737341529164), np.float64(-88.5)), (np.float64(39.248737341529164), np.float64(-88.5)), (np.float64(106.5), np.float64(-21.248737341529164)), (np.float64(106.5), np.float64(21.248737341529164)), (np.float64(39.248737341529164), np.float64(88.5)), (np.float64(15.0), np.float64(88.5))]), (np.float64(8.0), 39, [(np.float64(-15.0), np.float64(123.5)), (np.float64(-53.74621202458749), np.float64(123.5)), (np.float64(-141.5), np.float64(35.74621202458749)), (np.float64(-141.5), np.float64(-35.74621202458749)), (np.float64(-53.74621202458749), np.float64(-123.5)), (np.float64(53.74621202458749), np.float64(-123.5)), (np.float64(141.5), np.float64(-35.74621202458749)), (np.float64(141.5), np.float64(35.74621202458749)), (np.float64(53.74621202458749), np.float64(123.5)), (np.float64(15.0), np.float64(123.5))]), (np.float64(8.0), 39, [(np.float64(-15.0), np.float64(158.5)), (np.float64(-68.24368670764582), np.float64(158.5)), (np.float64(-176.5), np.float64(50.24368670764582)), (np.float64(-176.5), np.float64(-50.24368670764582)), (np.float64(-68.24368670764582), np.float64(-158.5)), (np.float64(68.24368670764582), np.float64(-158.5)), (np.float64(176.5), np.float64(-50.24368670764582)), (np.float64(176.5), np.float64(50.24368670764582)), (np.float64(68.24368670764582), np.float64(158.5)), (np.float64(15.0), np.float64(158.5))]), (8, 139, [(19.0, 84.5), (19.0, 232.5)])]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Replace 'your_data.pkl' with the path to your .pkl file\n",
    "file_path = '/rdf/shared/design_automation/Data_EMX/XFMRParallel_2504/SEG/10.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    loaded_list = pickle.load(f)\n",
    "\n",
    "totalD = loaded_list[0]\n",
    "totalU = loaded_list[1]\n",
    "print(totalD)\n",
    "print(len(totalD))\n",
    "print(totalU)\n",
    "print(len(totalU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a91ee4-5865-41fc-b5d4-9992de50c683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-21.5, -232.5), (-21.5, -64.5), (np.float64(-15.0), np.float64(-71.0)), (np.float64(-32.0), np.float64(-71.0)), (np.float64(-89.0), np.float64(-14.0)), (np.float64(-89.0), np.float64(14.0)), (np.float64(-31.999999999999996), np.float64(71.0)), (np.float64(32.0), np.float64(71.0)), (np.float64(89.0), np.float64(14.0)), (np.float64(89.0), np.float64(-14.0)), (np.float64(31.999999999999996), np.float64(-71.0)), (np.float64(15.0), np.float64(-71.0)), (np.float64(-15.0), np.float64(-106.0)), (np.float64(-46.49747468305833), np.float64(-106.0)), (np.float64(-124.0), np.float64(-28.497474683058336)), (np.float64(-123.99999999999999), np.float64(28.497474683058336)), (np.float64(-46.49747468305833), np.float64(106.0)), (np.float64(46.49747468305833), np.float64(106.0)), (np.float64(124.0), np.float64(28.497474683058336)), (np.float64(123.99999999999999), np.float64(-28.497474683058336)), (np.float64(46.49747468305833), np.float64(-106.0)), (np.float64(15.0), np.float64(-106.0)), (np.float64(-15.0), np.float64(-141.0)), (np.float64(-60.99494936611666), np.float64(-141.0)), (np.float64(-159.0), np.float64(-42.99494936611666)), (np.float64(-159.0), np.float64(42.99494936611666)), (np.float64(-60.99494936611665), np.float64(141.0)), (np.float64(60.99494936611666), np.float64(141.0)), (np.float64(159.0), np.float64(42.99494936611666)), (np.float64(159.0), np.float64(-42.99494936611666)), (np.float64(60.99494936611665), np.float64(-141.0)), (np.float64(15.0), np.float64(-141.0)), (21.5, -64.5), (21.5, -232.5)]\n",
      "34\n",
      "(34, 2)\n",
      "[(-21.5, -232.5), (-21.5, -64.5), (np.float64(-15.0), np.float64(-71.0)), (np.float64(-32.0), np.float64(-71.0)), (np.float64(-89.0), np.float64(-14.0)), (np.float64(-89.0), np.float64(14.0)), (np.float64(-31.999999999999996), np.float64(71.0)), (np.float64(32.0), np.float64(71.0)), (np.float64(89.0), np.float64(14.0)), (np.float64(89.0), np.float64(-14.0)), (np.float64(31.999999999999996), np.float64(-71.0)), (np.float64(15.0), np.float64(-71.0)), (np.float64(-15.0), np.float64(-106.0)), (np.float64(-46.49747468305833), np.float64(-106.0)), (np.float64(-124.0), np.float64(-28.497474683058336)), (np.float64(-123.99999999999999), np.float64(28.497474683058336)), (np.float64(-46.49747468305833), np.float64(106.0)), (np.float64(46.49747468305833), np.float64(106.0)), (np.float64(124.0), np.float64(28.497474683058336)), (np.float64(123.99999999999999), np.float64(-28.497474683058336)), (np.float64(46.49747468305833), np.float64(-106.0)), (np.float64(15.0), np.float64(-106.0)), (np.float64(-15.0), np.float64(-141.0)), (np.float64(-60.99494936611666), np.float64(-141.0)), (np.float64(-159.0), np.float64(-42.99494936611666)), (np.float64(-159.0), np.float64(42.99494936611666)), (np.float64(-60.99494936611665), np.float64(141.0)), (np.float64(60.99494936611666), np.float64(141.0)), (np.float64(159.0), np.float64(42.99494936611666)), (np.float64(159.0), np.float64(-42.99494936611666)), (np.float64(60.99494936611665), np.float64(-141.0)), (np.float64(15.0), np.float64(-141.0)), (21.5, -64.5), (21.5, -232.5)]\n",
      "34\n",
      "(34, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "node_D = []\n",
    "for item in totalD:\n",
    "    wid = item[0]\n",
    "    layer = item[1]\n",
    "    for node in item[2]:\n",
    "        node_D.append(node)\n",
    "print(node_D)\n",
    "print(len(node_D))\n",
    "print(np.shape(node_D))\n",
    "node_D = list(dict.fromkeys(node_D))\n",
    "print(node_D)\n",
    "print(len(node_D))\n",
    "print(np.shape(node_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fb7b036-439b-484f-bd98-13a1eb881f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-19.0, 232.5), (-19.0, 84.5), (np.float64(-15.0), np.float64(88.5)), (np.float64(-39.248737341529164), np.float64(88.5)), (np.float64(-106.5), np.float64(21.248737341529164)), (np.float64(-106.5), np.float64(-21.248737341529164)), (np.float64(-39.248737341529164), np.float64(-88.5)), (np.float64(39.248737341529164), np.float64(-88.5)), (np.float64(106.5), np.float64(-21.248737341529164)), (np.float64(106.5), np.float64(21.248737341529164)), (np.float64(39.248737341529164), np.float64(88.5)), (np.float64(15.0), np.float64(88.5)), (np.float64(-15.0), np.float64(123.5)), (np.float64(-53.74621202458749), np.float64(123.5)), (np.float64(-141.5), np.float64(35.74621202458749)), (np.float64(-141.5), np.float64(-35.74621202458749)), (np.float64(-53.74621202458749), np.float64(-123.5)), (np.float64(53.74621202458749), np.float64(-123.5)), (np.float64(141.5), np.float64(-35.74621202458749)), (np.float64(141.5), np.float64(35.74621202458749)), (np.float64(53.74621202458749), np.float64(123.5)), (np.float64(15.0), np.float64(123.5)), (np.float64(-15.0), np.float64(158.5)), (np.float64(-68.24368670764582), np.float64(158.5)), (np.float64(-176.5), np.float64(50.24368670764582)), (np.float64(-176.5), np.float64(-50.24368670764582)), (np.float64(-68.24368670764582), np.float64(-158.5)), (np.float64(68.24368670764582), np.float64(-158.5)), (np.float64(176.5), np.float64(-50.24368670764582)), (np.float64(176.5), np.float64(50.24368670764582)), (np.float64(68.24368670764582), np.float64(158.5)), (np.float64(15.0), np.float64(158.5)), (19.0, 84.5), (19.0, 232.5)]\n",
      "34\n",
      "(34, 2)\n",
      "[(-19.0, 232.5), (-19.0, 84.5), (np.float64(-15.0), np.float64(88.5)), (np.float64(-39.248737341529164), np.float64(88.5)), (np.float64(-106.5), np.float64(21.248737341529164)), (np.float64(-106.5), np.float64(-21.248737341529164)), (np.float64(-39.248737341529164), np.float64(-88.5)), (np.float64(39.248737341529164), np.float64(-88.5)), (np.float64(106.5), np.float64(-21.248737341529164)), (np.float64(106.5), np.float64(21.248737341529164)), (np.float64(39.248737341529164), np.float64(88.5)), (np.float64(15.0), np.float64(88.5)), (np.float64(-15.0), np.float64(123.5)), (np.float64(-53.74621202458749), np.float64(123.5)), (np.float64(-141.5), np.float64(35.74621202458749)), (np.float64(-141.5), np.float64(-35.74621202458749)), (np.float64(-53.74621202458749), np.float64(-123.5)), (np.float64(53.74621202458749), np.float64(-123.5)), (np.float64(141.5), np.float64(-35.74621202458749)), (np.float64(141.5), np.float64(35.74621202458749)), (np.float64(53.74621202458749), np.float64(123.5)), (np.float64(15.0), np.float64(123.5)), (np.float64(-15.0), np.float64(158.5)), (np.float64(-68.24368670764582), np.float64(158.5)), (np.float64(-176.5), np.float64(50.24368670764582)), (np.float64(-176.5), np.float64(-50.24368670764582)), (np.float64(-68.24368670764582), np.float64(-158.5)), (np.float64(68.24368670764582), np.float64(-158.5)), (np.float64(176.5), np.float64(-50.24368670764582)), (np.float64(176.5), np.float64(50.24368670764582)), (np.float64(68.24368670764582), np.float64(158.5)), (np.float64(15.0), np.float64(158.5)), (19.0, 84.5), (19.0, 232.5)]\n",
      "34\n",
      "(34, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "node_D = []\n",
    "for item in totalU:\n",
    "    wid = item[0]\n",
    "    layer = item[1]\n",
    "    for node in item[2]:\n",
    "        node_D.append(node)\n",
    "print(node_D)\n",
    "print(len(node_D))\n",
    "print(np.shape(node_D))\n",
    "node_D = list(dict.fromkeys(node_D))\n",
    "print(node_D)\n",
    "print(len(node_D))\n",
    "print(np.shape(node_D))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8276c9c4-ba34-4ec4-afb9-3a7c44fa1e85",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e15ec4b0-b539-4d8b-aabd-39da209ce304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the targeted max frequency\n",
    "fmax = 200\n",
    "fstart = 0\n",
    "\n",
    "#data loading function for images and tabular- derived from CNN path, same function used for MLP\n",
    "#define function for loading images and s-parameters\n",
    "#\"path\" is the path for images of transformer layout\n",
    "#\"label\" is the path for labels. In this case, labels are S-parameter associated with each images \n",
    "def load_images_from_path(path, label,length):\n",
    "    images = []\n",
    "    labels = []\n",
    "    error = []\n",
    "    for kk in range(length):\n",
    "        #load images\n",
    "        img = image.load_img(path+str(kk)+'.png', target_size=(300, 300, 3))\n",
    "        images.append(image.img_to_array(img)[38:262,38:262,:])#trim the ground margin\n",
    "\n",
    "        #load S-parameters\n",
    "        #According to symmerty, only S11,S12,S13,S14,S33,S34 are considered\n",
    "        #Each S-parameter has real and imaginary parts, and hence 12 real values in total.\n",
    "        results = []\n",
    "        datafile = label+str(kk)+\".s4p\"\n",
    "        spt = rf.Network(datafile)\n",
    "        step = int(1e9/(spt.f[1]-spt.f[0]))\n",
    "        for k in range(fmax+1):\n",
    "            i = int((k+fstart)*step/2)\n",
    "            results.append([spt.s[i][0][0].real,spt.s[i][0][0].imag,\n",
    "                            spt.s[i][0][1].real,spt.s[i][0][1].imag,\n",
    "                            spt.s[i][0][2].real,spt.s[i][0][2].imag,\n",
    "                            spt.s[i][0][3].real,spt.s[i][0][3].imag,\n",
    "                            spt.s[i][2][2].real,spt.s[i][2][2].imag,\n",
    "                            spt.s[i][2][3].real,spt.s[i][2][3].imag])     \n",
    "        labels.append(results)\n",
    "        sp = np.array(results)\n",
    "        invalid = False\n",
    "        '''\n",
    "        for k in range(len(sp)-3):\n",
    "            for x in range(6):\n",
    "                if (np.square(sp[k][2*x])+np.square(sp[k][2*x+1]))>1.02:\n",
    "                    invalid = True\n",
    "                \n",
    "            for x in range(12):\n",
    "                if abs(sp[k][x]+sp[k+2][x]-2*sp[k+1][x])>0.03:\n",
    "                    invalid = True\n",
    "                    '''\n",
    "        if (invalid):\n",
    "            error.append(kk)\n",
    "    return images, labels,error\n",
    "\n",
    "def show_images(images):\n",
    "    fig, axes = plt.subplots(1, 8, figsize=(20, 20), subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i] / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab77d29a-20a3-4d1b-a2f4-3c32db980a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac508a07-b24a-4d04-b60e-6a3974a188d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skrf as rf\n",
    "from scipy.spatial import distance\n",
    "from scipy.linalg import eigh\n",
    "#define the targeted max frequency\n",
    "fmax = 200\n",
    "#define function for loading ind and s-parameters\n",
    "#\"path\" is the path for ind of transformer layout\n",
    "#\"label\" is the path for labels. In this case, labels are S-parameter associated with each ind \n",
    "def load_ind_from_path(path, label,length):\n",
    "    ind_a = []\n",
    "    ind_b = []\n",
    "    labels = []\n",
    "    error = []\n",
    "    for kk in range(length):\n",
    "        result_a = []\n",
    "        result_b = []\n",
    "        #load ind\n",
    "        file_path = (path+str(kk)+'.pkl')\n",
    "        with open(file_path, 'rb') as f:\n",
    "            loaded_list = pickle.load(f)\n",
    "        totalD = loaded_list[0]\n",
    "        node_D = []\n",
    "        for item in totalD:\n",
    "            wid = item[0]\n",
    "            layer = item[1]\n",
    "            for node in item[2]:\n",
    "                node_D.append(node)\n",
    "        #print(np.shape(node_D))\n",
    "        node_D = list(dict.fromkeys(node_D))\n",
    "        #print(np.shape(node_D))\n",
    "        inda_node = np.array(node_D)\n",
    "        #print(np.shape(inda_node))\n",
    "        totalU = loaded_list[1]\n",
    "        node_U = []\n",
    "        for item in totalD:\n",
    "            wid = item[0]\n",
    "            layer = item[1]\n",
    "            for node in item[2]:\n",
    "                node_U.append(node)\n",
    "        node_U = list(dict.fromkeys(node_U))\n",
    "        indb_node = np.array(node_U)\n",
    "        dst_a = np.zeros(len(inda_node)-1)\n",
    "        aj_a = np.zeros((len(inda_node),len(inda_node)))\n",
    "        dst_b = np.zeros(len(indb_node)-1)\n",
    "        aj_b = np.zeros((len(indb_node),len(indb_node)))\n",
    "        \n",
    "        for x in range(len(inda_node)-1):\n",
    "            dst = distance.euclidean(inda_node[x],inda_node[x+1])\n",
    "            dst_a[x] = dst\n",
    "            aj_a[x][x+1] = dst_a[x]\n",
    "            aj_a[x+1][x] = dst_a[x]\n",
    "        \n",
    "        eigenvalues, eigenvectors = eigh(aj_a)\n",
    "        \n",
    "        for x in range(len(inda_node)-1):\n",
    "            if(inda_node[x+1][0]==15) or (inda_node[x+1][0]==-15):\n",
    "                continue\n",
    "            result_a.append([inda_node[x][0]/100,inda_node[x][1]/100,inda_node[x+1][0]/100,inda_node[x+1][1]/100,dst_a[x]/100,0,np.sum(eigenvectors[x][1]+eigenvectors[x+1][1])])\n",
    "        ind_a.append(result_a)\n",
    "        \n",
    "        for x in range(len(indb_node)-1):\n",
    "            dst = distance.euclidean(indb_node[x],indb_node[x+1])\n",
    "            dst_b[x] = dst\n",
    "            aj_b[x][x+1] = dst_b[x]\n",
    "            aj_b[x+1][x] = dst_b[x]\n",
    "        \n",
    "        eigenvalues, eigenvectors = eigh(aj_b)\n",
    "        \n",
    "        for x in range(len(indb_node)-1):\n",
    "            if(indb_node[x+1][0]==15) or (indb_node[x+1][0]==-15):\n",
    "                continue\n",
    "            result_b.append([indb_node[x][0]/100,indb_node[x][1]/100,indb_node[x+1][0]/100,indb_node[x+1][1]/100,dst_b[x]/100,0,np.sum(eigenvectors[x][1]+eigenvectors[x+1][1])])\n",
    "        ind_b.append(result_b)\n",
    "        #load S-parameters\n",
    "        #According to symmerty, only S11,S12,S13,S14,S33,S34 are considered\n",
    "        #Each S-parameter has real and imaginary parts, and hence 12 real values in total.\n",
    "        results = []\n",
    "        datafile = label+str(kk)+\".s4p\"\n",
    "        spt = rf.Network(datafile)\n",
    "        step = int(1e9/(spt.f[1]-spt.f[0]))\n",
    "        for k in range(fmax+1):\n",
    "            i = k*step\n",
    "            results.append([spt.s[i][0][0].real,spt.s[i][0][0].imag,\n",
    "                            spt.s[i][0][1].real,spt.s[i][0][1].imag,\n",
    "                            spt.s[i][0][2].real,spt.s[i][0][2].imag,\n",
    "                            spt.s[i][0][3].real,spt.s[i][0][3].imag,\n",
    "                            spt.s[i][2][2].real,spt.s[i][2][2].imag,\n",
    "                            spt.s[i][2][3].real,spt.s[i][2][3].imag])     \n",
    "        sp = np.array(results)\n",
    "        invalid = False\n",
    "        '''\n",
    "        for k in range(len(sp)-3):\n",
    "            for x in range(12):\n",
    "                if abs(sp[k][x]+sp[k+2][x]-2*sp[k+1][x])>0.03:\n",
    "                    invalid = True\n",
    "                    '''\n",
    "        if (invalid):\n",
    "            error.append(kk)\n",
    "        labels.append(results) \n",
    "    return ind_a,ind_b, labels,error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75948a55-aba5-4c26-8bd7-0a7a22fe9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_path(path):\n",
    "    logfile = open(path+\"log.txt\",\"r\")\n",
    "    listall = list(map(lambda item: list(map(lambda jtem: eval(jtem.strip()), item[1:-1].split(','))), logfile.read().strip().split('\\n')))\n",
    "    tabular_data = np.array(listall)\n",
    "\n",
    "#load images and s-parameters\n",
    "    length = len(tabular_data)\n",
    "    images, labels,error = load_images_from_path(path+\"PNG/\", path+\"SPData/\",length) #cnn, mlp data load\n",
    "    ind_a, ind_b, bang,bong = load_ind_from_path(path+\"SEG/\", path+\"SPData/\",length) #GT data load\n",
    "\n",
    "    \n",
    "    show_images(images)\n",
    "    srf = np.load(path+\"/srf.npy\")\n",
    "\n",
    "    for kk in range(length):\n",
    "    #select 1 turn and 1 turn transformers\n",
    "        if kk in error:\n",
    "            continue   \n",
    "        if((tabular_data[kk,4]!=0) and (tabular_data[kk,5]!=0)):\n",
    "       #geoparas only includes the variable parameters in our designs. \n",
    "       #Constants at this design stages are excluded \n",
    "            temp = next((item for item in reversed(tabular_data) if item[14] == kk), None)\n",
    "            geoparas.append(np.array(temp)[[0,1,2,3,4,7,8,9,13]])\n",
    "            images11.append(images[kk])\n",
    "            labels11.append(labels[kk])\n",
    "            srf_list.append(srf[kk])\n",
    "        if((tabular_data[kk,2]!=0) or (tabular_data[kk,3]!=0)):\n",
    "       #b only includes the variable parameters in our designs. \n",
    "       #Constants at this design stages are excluded\n",
    "            for i in range(len(ind_a[kk])):\n",
    "                ind_a[kk][i][5] = tabular_data[kk][4]\n",
    "            for i in range(len(ind_b[kk])):\n",
    "                ind_b[kk][i][5] = tabular_data[kk][4]\n",
    "            ind_a11.append(np.array(ind_a[kk]))\n",
    "            ind_b11.append(np.array(ind_b[kk])) #potential cause of issue if comes up in future - different label lists for graph and cnn\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9204e104-16b6-46ba-8a82-afb74d4ba467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "#geometrical parameters stored in log.txt\n",
    "#order of geopara (radiusA,radiusB,turnsA,turnsB,openA,openB,outA,outB,extA,extB,ratio,outbound)\n",
    "geoparas = []\n",
    "images11 = []\n",
    "labels11 = []\n",
    "ind_a11 = []\n",
    "ind_b11 = []\n",
    "srf_list = []\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        path = \"/rdf/shared/design_automation/Data_EMX/XFMRParallel_2504\"+\"/\"\n",
    "        load_from_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b54dd5-b3ef-4cd8-8bf2-29e12b1a3727",
   "metadata": {},
   "source": [
    "### Unimodal Encoder Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068665f-8771-47e7-921c-378f237b774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schematic (CNN)\n",
    "class SchemEncoder(nn.Module):\n",
    "    def __init__(self, fmax):\n",
    "        super(SchemEncoder, self).__init__()\n",
    "        \n",
    "        # Define the CNN part for input 1 (224x224x3)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, n_channel, kernel_size=3, stride=1, padding=1),  # (224, 224, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (112, 112, 32)\n",
    "            nn.Conv2d(n_channel, n_channel*2, kernel_size=3, stride=1, padding=1),  # (112, 112, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (56, 56, 128)\n",
    "            nn.Conv2d(n_channel*2, n_channel*4, kernel_size=3, stride=1, padding=1),  # (56, 56, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (28, 28, 128)\n",
    "            nn.Conv2d(n_channel*4, n_channel*4, kernel_size=3, stride=1, padding=1),  # (28, 28, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (14, 14, 128)\n",
    "            nn.Conv2d(n_channel*4, n_channel*4, kernel_size=3, stride=1, padding=1),  # (14, 14, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (7, 7, 128)\n",
    "            nn.Flatten(),  # Flatten to (7 * 7 * 128)\n",
    "            nn.Linear(7 * 7 * n_channel*4, 512),  # Fully connected layer\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Define the output layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, fmax * 12),  # Concatenate with input 2 (12 features)\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.reshape = nn.Unflatten(1, (fmax, 12))  # Reshape to (fmax, 12)\n",
    "\n",
    "    def forward(self, x1):\n",
    "        # Process input 1 through CNN\n",
    "        x = self.cnn(x1)\n",
    "        # Concatenate with input 2\n",
    "        #x = torch.cat((x1, x2), dim=1)\n",
    "        # Fully connected layer and reshape\n",
    "        x = self.fc(x)\n",
    "        x = self.reshape(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add83ab7-d843-40bf-952d-2f68c3059fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph (Graph Transformer)\n",
    "\n",
    "def get_activation(activation_str):\n",
    "    \"\"\"Map string to activation function.\"\"\"\n",
    "    if activation_str.lower() == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation_str.lower() == \"gelu\":\n",
    "        return F.gelu\n",
    "    # Add more activations if needed\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation: {activation_str}\")\n",
    "\n",
    "class GraphTransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    One Transformer encoder block, matching the structure of:\n",
    "      1) Multi-head self-attention (with dropout)\n",
    "      2) Feed-forward network (two linear layers + activation + dropout)\n",
    "      3) LayerNorm + residual connections\n",
    "      4) Optional \"pre-norm\" (norm_first=True) vs \"post-norm\" (norm_first=False)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size,\n",
    "        num_attention_heads,\n",
    "        intermediate_size,\n",
    "        activation=\"relu\", #check with houbo which activation is the best for GT block performance\n",
    "        dropout_rate=0.0,\n",
    "        attention_dropout_rate=0.0,\n",
    "        use_bias=False,\n",
    "        norm_first=True,\n",
    "        norm_epsilon=1e-6,\n",
    "        intermediate_dropout=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.attention_dropout_rate = attention_dropout_rate\n",
    "        self.use_bias = use_bias\n",
    "        self.norm_first = norm_first\n",
    "        self.norm_epsilon = norm_epsilon\n",
    "        self.intermediate_dropout = intermediate_dropout\n",
    "\n",
    "        # ---- Self-Attention ----\n",
    "        # nn.MultiheadAttention expects shape: [seq_len, batch_size, embed_dim]\n",
    "        # bias = `use_bias` is not directly exposed in nn.MultiheadAttention;\n",
    "        # PyTorch always learns a bias in the projection layers. If you want\n",
    "        # to remove bias, you must create a custom multi-head attention layer.\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_attention_heads,\n",
    "            dropout=attention_dropout_rate,\n",
    "            batch_first=False,  # We'll reshape manually\n",
    "            # PyTorch multi-head attention includes biases by default.\n",
    "            # For a strictly \"no-bias\" version, you'd need a custom approach.\n",
    "        )\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "        self.attention_layer_norm = nn.LayerNorm(hidden_size, eps=norm_epsilon)\n",
    "\n",
    "        # ---- Feed-Forward Network (FFN) ----\n",
    "        self.intermediate_dense = nn.Linear(hidden_size, intermediate_size, bias=use_bias)\n",
    "        self.intermediate_act_fn = get_activation(activation)\n",
    "        self.intermediate_dropout_layer = nn.Dropout(intermediate_dropout)\n",
    "\n",
    "        self.output_dense = nn.Linear(intermediate_size, hidden_size, bias=use_bias)\n",
    "        self.output_dropout = nn.Dropout(dropout_rate)\n",
    "        self.output_layer_norm = nn.LayerNorm(hidden_size, eps=norm_epsilon)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          hidden_states: Tensor of shape [batch_size, seq_len, hidden_size].\n",
    "          attention_mask: Optional tensor for attention, expected shape\n",
    "              [batch_size, seq_len, seq_len] with 0 for valid positions and\n",
    "              -inf (or large negative) for masked positions, or a boolean mask.\n",
    "              This may need to be adapted depending on how you've constructed\n",
    "              your mask. \n",
    "        Returns:\n",
    "          hidden_states: Tensor of shape [batch_size, seq_len, hidden_size].\n",
    "        \"\"\"\n",
    "\n",
    "        # --- Self-Attention block ---\n",
    "        # If norm_first, we layer-norm before attention; otherwise after\n",
    "        residual = hidden_states\n",
    "        if self.norm_first:\n",
    "            hidden_states = self.attention_layer_norm(hidden_states)\n",
    "\n",
    "        # Reshape hidden_states from [batch, seq, dim] to [seq, batch, dim]\n",
    "        hidden_states_t = hidden_states.transpose(0, 1)\n",
    "\n",
    "        # Convert mask if needed: PyTorch expects shape [seq_len, seq_len] or \n",
    "        # [batch_size * num_heads, seq_len, seq_len]. \n",
    "        # A simple approach is to expand so shape [batch, 1, seq, seq].\n",
    "        # Then internally PyTorch may broadcast it properly, or you\n",
    "        # can pass `attn_mask=some_mask` that is [seq, seq]. \n",
    "        # Here is an example that transforms the user’s [batch, seq, seq] \n",
    "        # into a float mask with -inf in invalid positions:\n",
    "        if attention_mask is not None:\n",
    "            # Suppose attention_mask=1 for valid, 0 for invalid, or the other way around.\n",
    "            # You may need to invert it, depending on how your mask is built.\n",
    "            # Here we assume \"1 = keep, 0 = mask out\".\n",
    "            attn_mask_pytorch = (1.0 - attention_mask) * -1e9\n",
    "            #print(attn_mask_pytorch.size())\n",
    "            # Expand dims if needed to [batch, 1, seq, seq], then flatten\n",
    "            # heads.  Alternatively, you can let PyTorch broadcast the shape.\n",
    "            # We’ll do a direct approach below:\n",
    "            #attn_mask_pytorch = attn_mask_pytorch.unsqueeze(1)  # [batch, 1, seq, seq]\n",
    "            #print(attn_mask_pytorch.size())\n",
    "        else:\n",
    "            attn_mask_pytorch = None\n",
    "\n",
    "        # Apply multi-head attention:\n",
    "        attn_output, _ = self.self_attention(\n",
    "            hidden_states_t,   # query\n",
    "            hidden_states_t,   # key\n",
    "            hidden_states_t,   # value\n",
    "            attn_mask=attn_mask_pytorch,\n",
    "        )\n",
    "\n",
    "        # Transpose back to [batch, seq, dim]\n",
    "        attn_output = attn_output.transpose(0, 1)\n",
    "\n",
    "        attn_output = self.attention_dropout(attn_output)\n",
    "        # Residual connection\n",
    "        hidden_states = residual + attn_output\n",
    "\n",
    "        if not self.norm_first:\n",
    "            hidden_states = self.attention_layer_norm(hidden_states)\n",
    "\n",
    "        # --- Feed Forward block ---\n",
    "        residual = hidden_states\n",
    "        if self.norm_first:\n",
    "            hidden_states = self.output_layer_norm(hidden_states)\n",
    "\n",
    "        # Intermediate (expand) + activation\n",
    "        hidden_states = self.intermediate_dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        hidden_states = self.intermediate_dropout_layer(hidden_states)\n",
    "\n",
    "        # Project back to hidden_size\n",
    "        hidden_states = self.output_dense(hidden_states)\n",
    "        hidden_states = self.output_dropout(hidden_states)\n",
    "\n",
    "        # Residual connection\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        if not self.norm_first:\n",
    "            hidden_states = self.output_layer_norm(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class GraphTransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stacks N TransformerEncoderBlock layers and applies a final layer norm\n",
    "    (to match the original Keras code which has 'output_normalization').\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers=6,\n",
    "        num_attention_heads=8,\n",
    "        intermediate_size=2048,\n",
    "        activation=\"relu\",\n",
    "        dropout_rate=0.0,\n",
    "        attention_dropout_rate=0.0,\n",
    "        use_bias=False,\n",
    "        norm_first=True,\n",
    "        norm_epsilon=1e-6,\n",
    "        intermediate_dropout=0.0,\n",
    "        hidden_size=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          num_layers: Number of encoder layers.\n",
    "          num_attention_heads: Number of attention heads.\n",
    "          intermediate_size: Dim of the FFN's hidden layer.\n",
    "          activation: Activation for the intermediate (FFN) layer.\n",
    "          dropout_rate: Dropout probability for the output of each sub-layer.\n",
    "          attention_dropout_rate: Dropout probability for the attention scores.\n",
    "          use_bias: Whether linear layers use bias.\n",
    "          norm_first: If True, apply layer norm before each sub-block.\n",
    "          norm_epsilon: Epsilon for layer norm.\n",
    "          intermediate_dropout: Dropout within the feed-forward 'intermediate' layers.\n",
    "          hidden_size: The input/output hidden size. If None, derive from input.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.attention_dropout_rate = attention_dropout_rate\n",
    "        self.use_bias = use_bias\n",
    "        self.norm_first = norm_first\n",
    "        self.norm_epsilon = norm_epsilon\n",
    "        self.intermediate_dropout = intermediate_dropout\n",
    "\n",
    "        # You can either require hidden_size to be passed explicitly,\n",
    "        # or you can infer it at runtime (by passing the first batch through).\n",
    "        if hidden_size is None:\n",
    "            raise ValueError(\n",
    "                \"You must specify 'hidden_size' (the input feature dimension).\"\n",
    "            )\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            GraphTransformerEncoderBlock(\n",
    "                hidden_size=hidden_size,\n",
    "                num_attention_heads=self.num_attention_heads,\n",
    "                intermediate_size=self.intermediate_size,\n",
    "                activation=self.activation,\n",
    "                dropout_rate=self.dropout_rate,\n",
    "                attention_dropout_rate=self.attention_dropout_rate,\n",
    "                use_bias=self.use_bias,\n",
    "                norm_first=self.norm_first,\n",
    "                norm_epsilon=self.norm_epsilon,\n",
    "                intermediate_dropout=self.intermediate_dropout,\n",
    "            ) for _ in range(self.num_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_normalization = nn.LayerNorm(hidden_size, eps=self.norm_epsilon)\n",
    "\n",
    "    def forward(self, encoder_inputs, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          encoder_inputs: shape [batch_size, seq_len, hidden_size].\n",
    "          attention_mask: shape [batch_size, seq_len, seq_len] or None.\n",
    "        Returns:\n",
    "          output shape [batch_size, seq_len, hidden_size].\n",
    "        \"\"\"\n",
    "        hidden_states = encoder_inputs\n",
    "\n",
    "        # Pass through each TransformerEncoderBlock\n",
    "        for i, layer in enumerate(self.encoder_layers):\n",
    "            #print(attention_mask.size())\n",
    "            hidden_states = layer(hidden_states, attention_mask=attention_mask)\n",
    "\n",
    "        # Final layer normalization (as in Keras code)\n",
    "        output_tensor = self.output_normalization(hidden_states)\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "#################################\n",
    "# Main Model\n",
    "#################################\n",
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self, transformer_encoder, fmax):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        self.transformer_encoder = transformer_encoder  # Use the pre-defined transformer model\n",
    "        self.fcl1 = nn.Linear(7, fdim)\n",
    "        self.fcl2 = nn.Linear(7, fdim)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(max_len*fdim*2, 512)  # Adjust input shape after concatenation 448*512\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 512)\n",
    "        self.fc5 = nn.Linear(512, 512)\n",
    "        self.out = nn.Linear(512, fmax * 12)\n",
    "        self.reshape = lambda x: x.view(-1, fmax, 12)  # Equivalent to `Reshape((fmax,12))` in Keras\n",
    "    def forward(self, inp1, inp2):\n",
    "        l1 = self.fcl1(inp1)\n",
    "        l2 = self.fcl2(inp2)\n",
    "        #print(l1.size())\n",
    "        #print(create_padding_mask(l1).size())\n",
    "        l1 = self.transformer_encoder(l1,create_padding_mask(inp1))\n",
    "        l2 = self.transformer_encoder(l2,create_padding_mask(inp2))\n",
    "\n",
    "        out = torch.cat((l1, l2), dim=1)  # Equivalent to `Concatenate()([l1, l2])`\n",
    "        out = self.flatten(out) #shape [8 2048]\n",
    "     #   print(\"1:\", out.shape)  \n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = torch.relu(self.fc2(out))\n",
    "        out = torch.relu(self.fc3(out))\n",
    "        out = torch.relu(self.fc4(out))\n",
    "        out = torch.relu(self.fc5(out))\n",
    "      #  print(\"2:\", out.shape)  \n",
    "        out = torch.tanh(self.out(out))  # Equivalent to `Dense(fmax*12, activation='tanh')`\n",
    "        out = self.reshape(out)\n",
    "       # print(\"3:\", out.shape)  \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fff182-b00b-40f4-961d-5ed0eb3c3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tabular (MLP)\n",
    "# PyTorch model equivalent to Keras Sequential model\n",
    "class TabEncoder(nn.Module):\n",
    "    def __init__(self, fband, input_size=16):\n",
    "        super(TabEncoder, self).__init__()\n",
    "        self.fband = fband\n",
    "        self.fc1 = nn.Linear(input_size, 512)  # Equivalent to Dense(512) in Keras\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 512)\n",
    "        self.out = nn.Linear(512, fband * 12)  # Output layer\n",
    "        self.tanh = nn.Tanh()  # Equivalent to 'tanh' activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # First Dense layer with ReLU activation\n",
    "        x = F.relu(self.fc2(x))  # Second Dense layer\n",
    "        x = F.relu(self.fc3(x))  # Third Dense layer\n",
    "        x = F.relu(self.fc4(x))  # Fourth Dense layer\n",
    "        x = self.out(x)          # Output layer\n",
    "        x = self.tanh(x)         # Tanh activation for output\n",
    "        #x = x.view(-1, self.fband, 12)  # Reshape to (fband, 12)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f8f729-2287-4c78-af6e-a38a27e04065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#token expansion - 1x512 -> 4x128\n",
    "class TokenExpander(nn.Module):\n",
    "    def __init__(self, in_dim=512, token_dim=512, num_tokens=6):\n",
    "        super(TokenExpander, self).__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.token_dim = token_dim\n",
    "\n",
    "        # Learn a projection from input to token sequence\n",
    "        self.token_proj = nn.Linear(in_dim, num_tokens * token_dim)\n",
    "\n",
    "        # Optional: normalization and non-linearity\n",
    "        self.norm = nn.LayerNorm(token_dim)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, in_dim)\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Project and reshape\n",
    "        tokens = self.token_proj(x)             # (B, num_tokens * token_dim)\n",
    "        tokens = tokens.view(B, self.num_tokens, self.token_dim)  # (B, num_tokens, token_dim)\n",
    "\n",
    "        # Normalize and activate each token\n",
    "        tokens = self.norm(tokens)\n",
    "        tokens = self.act(tokens)\n",
    "\n",
    "        return tokens  # ready for self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b77587-791f-454d-b480-25deff13b789",
   "metadata": {},
   "source": [
    "### Full Multimodal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80e82be-0713-4326-980f-daa1fa313e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to freeze parameters for unimodal heads (non-joint training scheme)\n",
    "\n",
    "def freeze(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88493ba9-1ddc-4aec-8f43-989925d5c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, pretrained_paths=None, num_latents=4, dim=512):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "\n",
    "        #unimodal heads\n",
    "        self.v1 = SchemEncoder(fband)# for schematic\n",
    "        self.v2 = GraphEncoder(GraphTransformerEncoder(intermediate_size=512,hidden_size=fdim), fmax=fband) # for graph\n",
    "        self.v3 = TabEncoder(fband)# for tabular\n",
    "\n",
    "\n",
    "        if pretrained_paths: #load pretrained weights\n",
    "            if 'schem_weights' in pretrained_paths:\n",
    "                self.v1.load_state_dict(torch.load(pretrained_paths['schem_weights']), strict =False)\n",
    "            if 'graph_weights' in pretrained_paths:\n",
    "                self.v2.load_state_dict(torch.load(pretrained_paths['graph_weights']), strict =False)\n",
    "            if 'tab_weights' in pretrained_paths:\n",
    "                self.v3.load_state_dict(torch.load(pretrained_paths['tab_weights']), strict =False)\n",
    "\n",
    "        \"\"\"\n",
    "        discard unnecessary layers and save parameters\n",
    "        \"\"\"\n",
    "        self.v1.fc = nn.Identity()\n",
    "        self.v1.reshape = nn.Identity()\n",
    "\n",
    "        self.v2.out = nn.Identity()\n",
    "        self.v2.reshape = nn.Identity()\n",
    "        \n",
    "        self.v3.out = nn.Identity()\n",
    "        self.v3.tanh = nn.Identity()\n",
    "\n",
    "        \"\"\"\n",
    "        Freeze parameters (comment out for joint training scheme)\n",
    "        \"\"\"\n",
    "        #freeze(self.v1)\n",
    "        #freeze(self.v2)\n",
    "        #freeze(self.v3)\n",
    "\n",
    "\n",
    "        self.v1_tokenize = TokenExpander()\n",
    "        self.v2_tokenize = TokenExpander()\n",
    "        self.v3_tokenize = TokenExpander()\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize auxillary unimodal transformers for fusion layers\n",
    "        \"\"\"\n",
    "        #encoder_base = nn.TransformerEncoderLayer(d_model=128, nhead=4) #d_model must be multiple of nhead      ***changed from (512,8) to (128,4)***\n",
    "        fusion_layers = 6\n",
    "        \n",
    "        # self.schem_aux = nn.TransformerEncoder(encoder_base, num_layers=fusion_layers)\n",
    "        # self.graph_aux = nn.TransformerEncoder(encoder_base, num_layers=fusion_layers)\n",
    "        # self.tab_aux = nn.TransformerEncoder(encoder_base, num_layers=fusion_layers)\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize Fusion Encoder and spectral head\n",
    "        \"\"\"\n",
    "        encoder_layers = []\n",
    "        for i in range(fusion_layers):\n",
    "\n",
    "            # Vanilla Transformer Encoder (use for full fine tuning)\n",
    "            \n",
    "            encoder_layers.append(nn.TransformerEncoderLayer(d_model = 512, nhead = 8))\n",
    "\n",
    "            # Frozen Transformer Encoder with AdaptFormer \n",
    "            #encoder_layers.append(AdaptFormer(num_latents=num_latents, dim=dim, schem_enc=self.schem_aux.blocks[i], graph_enc=self.graph_aux.blocks[i], tab_enc=self.tab_aux.blocks[i]))\n",
    "             \n",
    "        self.fusion_blocks = nn.Sequential(*encoder_layers)\n",
    "\n",
    "        #add normalization of bottlenecks maybe?\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        \n",
    "        # spectral head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512,512), #upward projection to d=512\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(512),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(512),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(512),\n",
    "            nn.Linear(512, fmax*12),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.reshape = lambda res: res.view(-1, fmax, 12)\n",
    "\n",
    "    def forward_encoder(self,tokens):     \n",
    "        # encoder forward pass\n",
    "        for blk in self.fusion_blocks:\n",
    "            fused = blk(tokens) #edit bottlenecklayer\n",
    "        return fused\n",
    "        \n",
    "    def forward(self, x, y_1, y_2, z):\n",
    "        #unimodal head outputs\n",
    "        x = self.v1(x)\n",
    "        y = self.v2(y_1,y_2)\n",
    "        z = self.v3(z)\n",
    "\n",
    "        x= self.v1_tokenize(x)\n",
    "        y= self.v2_tokenize(y)\n",
    "        z= self.v3_tokenize(z)\n",
    "\n",
    "        tokens = torch.cat([x,y,z], dim =1)\n",
    "        #fusion transformer encoders\n",
    "        tokens = self.forward_encoder(tokens)\n",
    "\n",
    "        x = tokens.mean(dim=1) #mean pooling\n",
    "        \n",
    "        #spectral head\n",
    "        x = self.head(x)\n",
    "        out = self.reshape(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c89009-bfc9-4012-9c85-d626ccc839bf",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a8d22-3440-481e-a121-3f0e9391613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph Hyperparameters\n",
    "import math\n",
    "\n",
    "# Set hyperparameters\n",
    "d_model = 32\n",
    "num_heads = 8\n",
    "dff = 256\n",
    "num_layers = 2\n",
    "max_seq_len = 32\n",
    "height = 7\n",
    "dropout_rate = 0.1\n",
    "\n",
    "#################################\n",
    "# Positional Encoding\n",
    "#################################\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, seq_len, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        positions = torch.arange(seq_len).unsqueeze(1).float()  # [seq_len, 1]\n",
    "        dims = torch.arange(d_model).unsqueeze(0).float()  # [1, d_model]\n",
    "\n",
    "        angle_rates = 1 / torch.pow(10000.0, (2 * (dims // 2)) / d_model)\n",
    "        angle_rads = positions * angle_rates\n",
    "\n",
    "        # Compute sine and cosine\n",
    "        sines = torch.sin(angle_rads[:, 0::2])\n",
    "        cosines = torch.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # Concatenate sine and cosine\n",
    "        pos_encoding = torch.cat([sines, cosines], dim=-1)  # [seq_len, d_model]\n",
    "        self.pos_encoding = pos_encoding.unsqueeze(0)  # [1, seq_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pos_encoding[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "#################################\n",
    "# Padding Mask\n",
    "#################################\n",
    "def create_padding_mask(x):\n",
    "    \"\"\"\n",
    "    Compute the padding mask:\n",
    "    - x: [batch_size, seq_len, height]\n",
    "    - mask: [batch_size, 1, 1, seq_len]\n",
    "    \"\"\"\n",
    "    summed = torch.sum(torch.abs(x), dim=2)  # [batch_size, seq_len]\n",
    "    mask = (summed == 0).float()  # 1.0 indicates padding\n",
    "     # Expand mask to match MultiheadAttention expected shape\n",
    "    mask = mask[:, None, None, :]  # Shape: [batch_size, 1, 1, seq_len]\n",
    "    \n",
    "    # Reshape to [batch_size * num_heads, seq_len, seq_len]\n",
    "    batch_size, seq_len, height = x.shape\n",
    "    mask = mask.expand(batch_size, num_heads, seq_len, seq_len)  # [batch_size, num_heads, seq_len, seq_len] \n",
    "    # Merge batch_size and num_heads into the first dimension\n",
    "    mask = mask.reshape(batch_size * num_heads, seq_len, seq_len)\n",
    "    return mask\n",
    "\n",
    "\n",
    "#################################\n",
    "# Example Usage\n",
    "#################################\n",
    "# Creating a batch of variable-length sequences with each element being a vector of length 'height'.\n",
    "# For demonstration:\n",
    "# Sequence 1: length=4\n",
    "# Sequence 2: length=7\n",
    "# Sequence 3: length=3\n",
    "# We'll pad them to length=7.\n",
    "\n",
    "def pad_sequence(seq_len, max_len=7):\n",
    "    length = seq_len.shape[0]\n",
    "    pad_len = max_len - length\n",
    "    return torch.cat([seq_len, torch.zeros(pad_len, height)], dim=0)\n",
    "\n",
    "max_len = max(len(seq) for seq in ind_b11)\n",
    "max_len = int(max_len) \n",
    "print(max_len)\n",
    "\n",
    "new_a = []\n",
    "print(ind_a11[0])\n",
    "for a in ind_a11:\n",
    "    padded_a = pad_sequence(torch.tensor(a, dtype=torch.float32),max_len)\n",
    "    new_a.append(padded_a.detach().numpy())\n",
    "print(np.shape(new_a))\n",
    "print(new_a[0])\n",
    "#transformer_encoder.eval()  # Set to evaluation mode\n",
    "#transformer_encoder = TransformerEncoder(num_layers, d_model, num_heads, dff, max_seq_len)\n",
    "#output = transformer_encoder(torch.stack(new_a))\n",
    "#print(\"Output shape:\", output.shape)  # [batch_size, seq_len, d_model]\n",
    "new_b = []\n",
    "for b in ind_b11:\n",
    "    padded_b = pad_sequence(torch.tensor(b, dtype=torch.float32),max_len)\n",
    "    new_b.append(padded_b.detach().numpy())\n",
    "print(np.shape(new_b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d13613-7df6-4ab1-a882-734e793afcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class item:\n",
    "  def __init__(self, image, geopara, a,b,label,srf):\n",
    "    self.image = image\n",
    "    self.geopara = geopara\n",
    "    self.a = a\n",
    "    self.b = b\n",
    "    self.label = label\n",
    "    self.srf = srf\n",
    "data = []\n",
    "for i in range(len(ind_a11)):\n",
    "    data.append(item(images11[i],geoparas[i],new_a[i],new_b[i],labels11[i],srf_list[i]))\n",
    "\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948bed2f-5edc-4300-8039-301520c8d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the split length for training, validation and test datasets\n",
    "length = len(images11)\n",
    "split1 = int(0.6*length)\n",
    "split2 = int(0.8*length)\n",
    "data_train = data[0:split1]\n",
    "data_valid = data[split1:split2]\n",
    "data_test = data[split2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06db3db0-96cf-403e-9114-14b4a57c01cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate the training, validation and test datasets \n",
    "train_images = []\n",
    "train_labels = []\n",
    "train_a = []\n",
    "train_b = []\n",
    "train_geoparas = []\n",
    "train_srf = []\n",
    "\n",
    "valid_images = []\n",
    "valid_labels = []\n",
    "valid_a = []\n",
    "valid_b = []\n",
    "valid_geoparas = []\n",
    "valid_srf = []\n",
    "\n",
    "test_images = []\n",
    "test_labels = []\n",
    "test_a = []\n",
    "test_b = []\n",
    "test_geoparas = []\n",
    "test_srf = []\n",
    "\n",
    "#The targeted frequency span up to fmax, with 1GHz step\n",
    "s_max = np.zeros((fmax,12))\n",
    "s_min = np.zeros((fmax,12))\n",
    "srange = np.zeros((fmax,12))\n",
    "\n",
    "\n",
    "for i in range(len(data_train)):\n",
    "    train_images.append(data_train[i].image)\n",
    "    train_geoparas.append(data_train[i].geopara)\n",
    "    train_a.append(data_train[i].a)\n",
    "    train_b.append(data_train[i].b)\n",
    "    train_labels.append(data_train[i].label)\n",
    "    train_srf.append(data_train[i].srf)\n",
    "x_train_img = np.array(train_images) / 255\n",
    "x_train_srf = np.array(train_srf)\n",
    "x_train_a = np.array(train_a) \n",
    "x_train_b = np.array(train_b)\n",
    "x_train_geopara = np.array(train_geoparas)\n",
    "y_train_encoded = np.array(train_labels)[:,1:fmax+1,:]\n",
    "\n",
    "\n",
    "for i in range(len(data_valid)):\n",
    "    valid_images.append(data_valid[i].image)\n",
    "    valid_geoparas.append(data_valid[i].geopara)\n",
    "    valid_a.append(data_valid[i].a)\n",
    "    valid_b.append(data_valid[i].b)\n",
    "    valid_labels.append(data_valid[i].label)\n",
    "    valid_srf.append(data_valid[i].srf)\n",
    "x_valid_img = np.array(valid_images) / 255\n",
    "x_valid_srf = np.array(valid_srf)\n",
    "x_valid_a = np.array(valid_a) \n",
    "x_valid_b = np.array(valid_b)\n",
    "x_valid_geopara = np.array(valid_geoparas)\n",
    "y_valid_encoded = np.array(valid_labels)[:,1:fmax+1,:]\n",
    "\n",
    "\n",
    "for i in range(len(data_test)):\n",
    "    test_images.append(data_test[i].image)\n",
    "    test_geoparas.append(data_test[i].geopara)\n",
    "    test_a.append(data_test[i].a)\n",
    "    test_b.append(data_test[i].b)\n",
    "    test_labels.append(data_test[i].label)\n",
    "    test_srf.append(data_test[i].srf)\n",
    "x_test_img = np.array(test_images) / 255\n",
    "x_test_srf = np.array(test_srf)\n",
    "x_test_a = np.array(test_a) \n",
    "x_test_b = np.array(test_b)\n",
    "x_test_geopara = np.array(test_geoparas)\n",
    "y_test_encoded = np.array(test_labels)[:,1:fmax+1,:]\n",
    "\n",
    "#normalize the s-parameters for each frequency point\n",
    "for z in range(fmax):\n",
    "    for i in range(12):\n",
    "        s_max[z,i] = max(max(y_train_encoded[:,z,i]),max(y_valid_encoded[:,z,i]),max(y_test_encoded[:,z,i]))\n",
    "        s_min[z,i] = min(min(y_train_encoded[:,z,i]),min(y_valid_encoded[:,z,i]),min(y_test_encoded[:,z,i]))\n",
    "        srange[z,i] = s_max[z,i]-s_min[z,i]\n",
    "    \n",
    "    for i in range(12):\n",
    "        y_train_encoded[:,z,i] = 2*(y_train_encoded[:,z,i]-s_min[z,i])/srange[z,i]-1\n",
    "\n",
    "    for i in range(12):\n",
    "        y_valid_encoded[:,z,i] = 2*(y_valid_encoded[:,z,i]-s_min[z,i])/srange[z,i]-1\n",
    "    \n",
    "    for i in range(12):\n",
    "        y_test_encoded[:,z,i] = 2*(y_test_encoded[:,z,i]-s_min[z,i])/srange[z,i]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46521f2f-2c2b-4599-b630-77a518cc96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "learning_rate = 1e-4\n",
    "epochs = 180\n",
    "\n",
    "nband = int(1)\n",
    "overlap = int(1)\n",
    "fband = int(fmax *overlap/ nband)  # Replace with your value for fmax\n",
    "bandslice = int(fband/overlap)\n",
    "fdim = 32\n",
    "#max_len = 7 #idk what this is yet\n",
    "n_channel = 16\n",
    "\n",
    "models_dir = '/home/ch106/Desktop/ASP_DAC2026/MxN/models'\n",
    "pretrained_paths = { #dictionary storing pretrained weights for unimodal encoders\n",
    "    'schem_weights': models_dir + '/' +'cnn_1band/0.pth',\n",
    "    'graph_weights': models_dir + '/' +'GT_1band/0.pth',\n",
    "    'tab_weights': models_dir + '/' +'mlp_1band/0.pth'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afdcb17-456d-4888-8cd2-56360588399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    Lf = torch.sqrt(torch.mean((y_true - y_pred) ** 2, dim=1))  # Compute RMSE across feature axis\n",
    "    #return torch.log(torch.mean(Lf))  # Compute log of mean RMSE\n",
    "    return (torch.mean(Lf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66e16b-fb81-4778-988b-548c1e2c2c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe9410-6277-4808-9a73-afc0c461c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, val_loader, loss_function):\n",
    "    model.eval()  # 1) inference mode\n",
    "    running_loss = 0.0\n",
    "    running_mae  = 0.0\n",
    "\n",
    "    with torch.no_grad():  # 2) no grads\n",
    "        for inp0, inp1, inp2, inp3, y_true in val_loader:\n",
    "            # same data movement\n",
    "            inp0, inp1, inp2, inp3, y_true = [\n",
    "                x.to(device) for x in (inp0, inp1, inp2, inp3, y_true)\n",
    "            ]\n",
    "\n",
    "            # forward only\n",
    "            y_pred = model(inp0, inp1, inp2, inp3)\n",
    "            loss   = loss_function(y_true, y_pred)\n",
    "            mae_val = torch.mean(torch.abs(y_true - y_pred))\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_mae  += mae_val.item()\n",
    "            # no backward(), no step(), no empty_cache()\n",
    "\n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    avg_mae  = running_mae  / len(val_loader)\n",
    "    return avg_loss, avg_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b747bfe-c407-4746-a949-f640e627ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model\n",
    "model = MultimodalModel(pretrained_paths=pretrained_paths).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "def lr_lambda(epoch):\n",
    "    return 0.95 ** (epoch / 100)  # 对应 Keras 的 0.95**(epoch/20)\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.995)\n",
    "\n",
    "# Loss function\n",
    "loss_function = custom_loss\n",
    "\n",
    "#initialize epoch start time\n",
    "\n",
    "\n",
    "# Metric for MAE (Mean Absolute Error)\n",
    "def mae(y_true, y_pred):\n",
    "    return torch.mean(torch.abs(y_true - y_pred))\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, optimizer, scheduler, loss_function,\n",
    "                num_epochs, train_loader, val_loader=None):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # ---- TRAIN ----\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_mae  = 0.0\n",
    "\n",
    "        for inp0, inp1, inp2, inp3, y_true in train_loader:\n",
    "            inp0, inp1, inp2, inp3, y_true = [\n",
    "                x.to(device) for x in (inp0, inp1, inp2, inp3, y_true)\n",
    "            ]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(inp0, inp1, inp2, inp3)\n",
    "            loss = loss_function(y_true, y_pred)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_mae  += torch.mean(torch.abs(y_true - y_pred)).item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_train_mae  = running_mae  / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # ---- VALIDATION ----\n",
    "        if val_loader is not None:\n",
    "            val_loss, val_mae = validate_one_epoch(model, val_loader, loss_function)\n",
    "            val_losses.append(val_loss)\n",
    "        else:\n",
    "            val_loss = val_mae = None\n",
    "\n",
    "        # ---- LOGGING ----\n",
    "        if epoch % 20 == 0 or epoch == 1 or epoch == num_epochs:\n",
    "            msg = (f\"Epoch [{epoch}/{num_epochs}]  \"\n",
    "                   f\"Train Loss: {avg_train_loss:.4f}, Train MAE: {avg_train_mae:.4f}\")\n",
    "            if val_loader is not None:\n",
    "                msg += f\"  Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}\"\n",
    "            print(msg)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal training time: {total_time/60:.2f} minutes\")\n",
    "\n",
    "    # ← plot losses\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    if val_loader is not None:\n",
    "        plt.plot(epochs, val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs. Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# Dummy data (replace with actual DataLoader)\n",
    "batch_size = 32\n",
    "\n",
    "xx = 0\n",
    "# Convert data to PyTorch tensors and move them to float32\n",
    "yout = torch.from_numpy(y_train_encoded[:, (xx * fband):(xx + 1) * fband]).type(torch.float32)\n",
    "xa = torch.from_numpy(x_train_a).type(torch.float32)  # Example graph a input\n",
    "xb = torch.from_numpy(x_train_b).type(torch.float32)  # Example graph b input\n",
    "x1 = torch.from_numpy(x_train_img)  # Example image input\n",
    "x1 = x1.permute(0, 3, 1, 2)\n",
    "x2 = torch.from_numpy(x_train_geopara).type(torch.float32)  # Example geoparam input\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(x1, xa, xb, x2, yout)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Convert data to PyTorch tensors and move them to float32\n",
    "yout_valid = torch.from_numpy(y_valid_encoded[:, (xx * fband):(xx + 1) * fband]).type(torch.float32)\n",
    "xa_valid = torch.from_numpy(x_valid_a).type(torch.float32)  # Example graph a input\n",
    "xb_valid = torch.from_numpy(x_valid_b).type(torch.float32)  # Example graph b input\n",
    "x1_valid = torch.from_numpy(x_valid_img)  # Example image input\n",
    "x1_valid = x1_valid.permute(0, 3, 1, 2)\n",
    "x2_valid = torch.from_numpy(x_valid_geopara).type(torch.float32)  # Example geoparam input\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(x1_valid, xa_valid, xb_valid, x2_valid, yout_valid)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Move tensors to device\n",
    "xa = xa.to(device)\n",
    "xb = xb.to(device)\n",
    "x1 = x1.to(device)\n",
    "x2 = x2.to(device)\n",
    "yout = yout.to(device)\n",
    "\n",
    "xa_valid = xa_valid.to(device)\n",
    "xb_valid = xb_valid.to(device)\n",
    "x1_valid = x1_valid.to(device)\n",
    "x2_valid = x2_valid.to(device)\n",
    "yout_valid = yout_valid.to(device)\n",
    "\n",
    "# Train the model\n",
    "print(f\"GPU memory before: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "train_model(model, optimizer, scheduler, loss_function, epochs, dataloader, val_dataloader)\n",
    "print(f\"GPU memory after: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd5780a-9ef5-4098-8b49-2a2343afdfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './weights/MMT_1band/'+str(xx) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7934fc-e241-4e2b-902e-c9f93f35aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5227515-71ac-42ab-b58b-a5173b1cbbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency forward loop\n",
    "def forward_loop():\n",
    "    for ff in range(nband - overlap):\n",
    "        xx = ff + 1\n",
    "    #print(f\"Processing dataset for frequency band {xx}\")\n",
    "\n",
    "    # Load model weights\n",
    "        model.load_state_dict(torch.load('./weights/MMT_1band/'+str(xx - 1) + '.pth'))  # Load saved weights\n",
    "        model.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "    # Prepare new dataset for this iteration\n",
    "        yout = torch.from_numpy(y_train_encoded[:, (xx * bandslice):(xx + overlap) * bandslice])\n",
    "        yout = yout.type(torch.float32).to(device)  # Move data to the device\n",
    "        dataset = torch.utils.data.TensorDataset(x1, xa, xb, x2, yout)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "    # Reinitialize optimizer and scheduler with the updated model parameters\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.995)\n",
    "\n",
    "    # Train the model on the new dataset\n",
    "        train_model(model, optimizer, scheduler, loss_function, epochs, dataloader)\n",
    "\n",
    "    # Save the updated model weights\n",
    "        torch.save(model.state_dict(), './weights/MMT_1band/'+str(xx) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bb16f8-7566-44f6-b290-8c867a698df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency backward loop\n",
    "def backward_loop():\n",
    "    for ff in range(nband - overlap):\n",
    "        xx = nband - ff - 1 - overlap\n",
    "        #print(f\"Processing dataset for frequency band {xx}\")\n",
    "\n",
    "    # Load model weights\n",
    "        model.load_state_dict(torch.load('./weights/MMT_1band/'+str(xx + 1) + '.pth'))  # Load saved weights\n",
    "        model.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "    # Prepare new dataset for this iteration\n",
    "        yout = torch.from_numpy(y_train_encoded[:, (xx * bandslice):(xx + overlap) * bandslice])\n",
    "        yout = yout.type(torch.float32).to(device)  # Move data to the device\n",
    "        dataset = torch.utils.data.TensorDataset(x1, xa, xb, x2, yout)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Reinitialize optimizer and scheduler with the updated model parameters\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.995)\n",
    "\n",
    "    # Train the model on the new dataset\n",
    "        train_model(model, optimizer, scheduler, loss_function, epochs, dataloader)\n",
    "\n",
    "    # Save the updated model weights\n",
    "        torch.save(model.state_dict(), './weights/MMT_1band/'+str(xx) + '.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3ce624-4150-44a9-8bbb-163212a4ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(ax):\n",
    "    batch_size = 32  # you can adjust this based on your GPU headroom\n",
    "\n",
    "    # prepare output arrays\n",
    "    test_prediction_raw = np.zeros((len(x_test_a), fmax * overlap, 12))\n",
    "    test_prediction     = np.zeros((len(x_test_a), fmax, 12))\n",
    "\n",
    "    # graph test data health check\n",
    "\n",
    "    # loop over each band model\n",
    "    for xx in range(nband + 1 - overlap):\n",
    "        # load weights and switch to eval\n",
    "        model.load_state_dict(torch.load(f'./weights/MMT_1band/{xx}.pth'))\n",
    "        #model.eval()\n",
    "\n",
    "        # batched inference for this band\n",
    "        band_preds = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(x_test_a), batch_size):\n",
    "                # slice numpy → torch.cuda tensors\n",
    "                a_batch     = torch.from_numpy(x_test_a[i:i+batch_size]).float().to(device)\n",
    "                b_batch     = torch.from_numpy(x_test_b[i:i+batch_size]).float().to(device)\n",
    "                schem_batch = (\n",
    "                    torch.from_numpy(x_test_img[i:i+batch_size])\n",
    "                         .float()\n",
    "                         .permute(0, 3, 1, 2)\n",
    "                         .to(device)\n",
    "                )\n",
    "                tab_batch   = torch.from_numpy(x_test_geopara[i:i+batch_size]).float().to(device)\n",
    "\n",
    "                # forward\n",
    "                out_batch = model(schem_batch, a_batch, b_batch, tab_batch)\n",
    "                band_preds.append(out_batch.cpu().numpy())\n",
    "\n",
    "        # concatenate all minibatches and store\n",
    "        band_preds = np.concatenate(band_preds, axis=0)\n",
    "        test_prediction_raw[:, fband * xx : fband * (xx + 1), :] = band_preds\n",
    "\n",
    "        # reset for next band\n",
    "        mae_all = np.zeros((fmax, 12))\n",
    "\n",
    "    # combine overlapping predictions exactly as before\n",
    "    mae_mean = np.zeros(fmax)\n",
    "    for xx in range(nband):\n",
    "        print(xx)\n",
    "        if xx < overlap:\n",
    "            for yy in range(1 + xx):\n",
    "                test_prediction[:, bandslice*xx:bandslice*(xx+1), :] += \\\n",
    "                    test_prediction_raw[:, fband*(xx-yy)+yy*bandslice : fband*(xx-yy)+(yy+1)*bandslice, :]\n",
    "            test_prediction[:, bandslice*xx:bandslice*(xx+1), :] /= (1 + xx)\n",
    "\n",
    "        elif (nband - xx) < overlap:\n",
    "            for yy in range(2 + xx - nband):\n",
    "                test_prediction[:, bandslice*xx:bandslice*(xx+1), :] += \\\n",
    "                    test_prediction_raw[:, fband*(xx+1-overlap-yy)+(overlap-1-yy)*bandslice \n",
    "                                             : fband*(xx+1-overlap-yy)+(overlap-yy)*bandslice, :]\n",
    "            test_prediction[:, bandslice*xx:bandslice*(xx+1), :] /= (2 + xx - nband)\n",
    "\n",
    "        else:\n",
    "            for yy in range(overlap):\n",
    "                test_prediction[:, bandslice*xx:bandslice*(xx+1), :] += \\\n",
    "                    test_prediction_raw[:, fband*(xx-yy)+yy*bandslice : fband*(xx-yy)+(yy+1)*bandslice, :]\n",
    "            test_prediction[:, bandslice*xx:bandslice*(xx+1), :] /= overlap\n",
    "\n",
    "    # compute MAE and plot\n",
    "    for z in range(fmax):\n",
    "        for i in range(12):\n",
    "            mae_all[z, i] = (\n",
    "                np.mean(\n",
    "                    np.abs(test_prediction[:, z, i] - y_test_encoded[:, z, i])\n",
    "                )\n",
    "                * 0.5\n",
    "                * srange[z, i]\n",
    "            )\n",
    "        mae_mean[z] = np.mean(mae_all[z, :])\n",
    "\n",
    "    ax.plot(range(fmax), mae_mean)\n",
    "    print(np.mean(mae_mean))\n",
    "    print(R_squared(torch.from_numpy(y_test_encoded),\n",
    "                    torch.from_numpy(test_prediction)))\n",
    "\n",
    "    return np.mean(mae_mean)\n",
    "\n",
    "    \n",
    "def R_squared(y, y_pred):\n",
    "    # Residual sum of squares\n",
    "    residual = torch.sum((y - y_pred) ** 2)\n",
    "    \n",
    "    # Total sum of squares\n",
    "    total = torch.sum((y - torch.mean(y)) ** 2)\n",
    "    \n",
    "    # Compute R-squared\n",
    "    r2 = 1.0 - (residual / total)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac9585d-5bca-4516-92a1-82eca9f42548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_srf(ax, batch_size=32):\n",
    "    # Preallocate output arrays\n",
    "    test_prediction_raw = np.zeros((len(x_test_a), fmax * overlap, 12))\n",
    "    test_prediction     = np.zeros((len(x_test_a), fmax, 12))\n",
    "\n",
    "    # Loop over each frequency band\n",
    "    for xx in range(nband + 1 - overlap):\n",
    "        # Load weights and switch to eval/inference\n",
    "        model.load_state_dict(torch.load(f'./weights/MMT_1band/{xx}.pth'))\n",
    "        #model.eval()\n",
    "\n",
    "        # Collect this band’s predictions in batches\n",
    "        band_preds = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(x_test_a), batch_size):\n",
    "                # Slice and move each modality to device\n",
    "                a_batch = (\n",
    "                    torch.from_numpy(x_test_a[i : i + batch_size])\n",
    "                         .float()\n",
    "                         .to(device)\n",
    "                )\n",
    "                b_batch = (\n",
    "                    torch.from_numpy(x_test_b[i : i + batch_size])\n",
    "                         .float()\n",
    "                         .to(device)\n",
    "                )\n",
    "                schem_batch = (\n",
    "                    torch.from_numpy(x_test_img[i : i + batch_size])\n",
    "                         .float()\n",
    "                         .permute(0, 3, 1, 2)\n",
    "                         .to(device)\n",
    "                )\n",
    "                tab_batch = (\n",
    "                    torch.from_numpy(x_test_geopara[i : i + batch_size])\n",
    "                         .float()\n",
    "                         .to(device)\n",
    "                )\n",
    "\n",
    "                # Forward pass\n",
    "                out_batch = model(schem_batch, a_batch, b_batch, tab_batch)\n",
    "                band_preds.append(out_batch.cpu().numpy())\n",
    "\n",
    "        # Concatenate and store for this band\n",
    "        band_preds = np.concatenate(band_preds, axis=0)\n",
    "        test_prediction_raw[:, fband * xx : fband * (xx + 1), :] = band_preds\n",
    "\n",
    "        # (reset placeholder, as in original)\n",
    "        mae_all = np.zeros((fmax, 12))\n",
    "\n",
    "    # Combine overlapping band predictions exactly as before\n",
    "    for xx in range(nband):\n",
    "        print(xx)\n",
    "        if xx < overlap:\n",
    "            for yy in range(1 + xx):\n",
    "                test_prediction[:, bandslice * xx : bandslice * (xx + 1), :] += (\n",
    "                    test_prediction_raw[\n",
    "                        :,\n",
    "                        fband * (xx - yy) + yy * bandslice : fband * (xx - yy) + (yy + 1) * bandslice,\n",
    "                        :\n",
    "                    ]\n",
    "                )\n",
    "            test_prediction[:, bandslice * xx : bandslice * (xx + 1), :] /= (1 + xx)\n",
    "\n",
    "        elif (nband - xx) < overlap:\n",
    "            for yy in range(2 + xx - nband):\n",
    "                test_prediction[:, bandslice * xx : bandslice * (xx + 1), :] += (\n",
    "                    test_prediction_raw[\n",
    "                        :,\n",
    "                        fband * (xx + 1 - overlap - yy) + (overlap - 1 - yy) * bandslice\n",
    "                          : fband * (xx + 1 - overlap - yy) + (overlap - yy) * bandslice,\n",
    "                        :\n",
    "                    ]\n",
    "                )\n",
    "            test_prediction[:, bandslice * xx : bandslice * (xx + 1), :] /= (2 + xx - nband)\n",
    "\n",
    "        else:\n",
    "            for yy in range(overlap):\n",
    "                test_prediction[:, bandslice * xx : bandslice * (xx + 1), :] += (\n",
    "                    test_prediction_raw[\n",
    "                        :,\n",
    "                        fband * (xx - yy) + yy * bandslice : fband * (xx - yy) + (yy + 1) * bandslice,\n",
    "                        :\n",
    "                    ]\n",
    "                )\n",
    "            test_prediction[:, bandslice * xx : bandslice * (xx + 1), :] /= overlap\n",
    "\n",
    "    # Compute SRF-normalized MAE\n",
    "    n_srf = 100\n",
    "    srf_mae_all = np.zeros((n_srf, 12))\n",
    "    srf_mae     = np.zeros(n_srf)\n",
    "    for z in range(n_srf):\n",
    "        for sample in range(len(x_test_a)):\n",
    "            freq_temp = int(min(fmax-1, z * test_srf[sample] * 2 * 2 / n_srf))\n",
    "            for i in range(12):\n",
    "                srf_mae_all[z, i] += (\n",
    "                    abs(test_prediction[sample, freq_temp, i] - y_test_encoded[sample, freq_temp, i])\n",
    "                    * 0.5\n",
    "                    * srange[freq_temp, i]\n",
    "                )\n",
    "        srf_mae_all[z] /= len(x_test_a)\n",
    "        srf_mae[z] = np.mean(srf_mae_all[z])\n",
    "\n",
    "    # Plot and report\n",
    "    ax.plot((2 / n_srf) * np.ones(n_srf) * np.arange(n_srf), srf_mae)\n",
    "    print(np.mean(srf_mae))\n",
    "    print(R_squared(torch.from_numpy(y_test_encoded),\n",
    "                    torch.from_numpy(test_prediction)))\n",
    "\n",
    "    return np.mean(srf_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dcab63-b99f-4423-a56f-a96332cbea59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "iteration = 5\n",
    "fig1, ax1 = plt.subplots()\n",
    "fig2, ax2 = plt.subplots()\n",
    "mae_trend = np.zeros(iteration)\n",
    "for ii in range(iteration):\n",
    "    epochs = epochs\n",
    "    forward_loop()\n",
    "    backward_loop()\n",
    "    print(ii)\n",
    "    mae_trend[ii]=prediction(ax1)\n",
    "    prediction_srf(ax2)\n",
    "ax1.set_xlabel('Frequency/GHz')\n",
    "ax1.set_ylabel('Average S-parameter mae')\n",
    "ax2.set_xlabel('Frequency Normalized to SRF')\n",
    "ax2.set_ylabel('Average S-parameter mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b7107-8e64-4c02-81f7-5ffd4243c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=[(1,3,224,224),(1, 32, 7),(1,32,7),(1,16)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19216023-6e28-4bb9-bb63-8c2e34c602dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
